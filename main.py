import os
import json
import pandas as pd
import requests
from geopy.geocoders import Nominatim
from geopy.distance import geodesic
from datetime import date, datetime, timedelta

# --- 1. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾— ---
PLACE_INPUT = os.environ.get("SEARCH_VENUE", "ã‚»ã‚­ã‚¹ã‚¤ãƒã‚¤ãƒ ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¢ãƒªãƒ¼ãƒŠ, 2026-03-09")
COND_INPUT = os.environ.get("SEARCH_COND", "ç¦ç…™,æœé£Ÿä»˜ã")
RAKUTEN_APP_ID = os.environ.get("RAKUTEN_APP_ID")

# --- 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆæœŸåŒ– ---
def initialize_data():
    with open("rakuten_area_class.json", "r", encoding="utf-8") as f:
        data = json.load(f)

    records = []
    for large_entry in data["areaClasses"]["largeClasses"]:
        l_info = large_entry["largeClass"][0]
        l_code, l_name = l_info["largeClassCode"], l_info["largeClassName"]
        if len(large_entry["largeClass"]) < 2: continue
        
        for middle_entry in large_entry["largeClass"][1]["middleClasses"]:
            m_info = middle_entry["middleClass"][0]
            m_code, m_name = m_info["middleClassCode"], m_info["middleClassName"]
            if len(middle_entry["middleClass"]) < 2: continue

            for small_entry in middle_entry["middleClass"][1]["smallClasses"]:
                s_info = small_entry["smallClass"][0]
                s_code, s_name = s_info["smallClassCode"], s_info["smallClassName"]

                # è©³ç´°ã‚¨ãƒªã‚¢ãŒã‚ã‚‹å ´åˆ
                if len(small_entry["smallClass"]) >= 2 and "detailClasses" in small_entry["smallClass"][1]:
                    for detail_entry in small_entry["smallClass"][1]["detailClasses"]:
                        d_class = detail_entry["detailClass"]
                        records.append({
                            "largeClassCode": l_code, "middleClassCode": m_code, "smallClassCode": s_code, 
                            "detailClassCode": d_class["detailClassCode"], "largeClassName": l_name, 
                            "middleClassName": m_name, "smallClassName": s_name, "detailClassName": d_class["detailClassName"]
                        })
                else:
                    records.append({
                        "largeClassCode": l_code, "middleClassCode": m_code, "smallClassCode": s_code, 
                        "detailClassCode": "", "largeClassName": l_name, "middleClassName": m_name, 
                        "smallClassName": s_name, "detailClassName": s_name
                    })
    
    rakuten_df = pd.DataFrame(records)
    
    try:
        gaz_df = pd.read_csv("gazetteer-of-japan.csv")[["kanji", "lat", "lng"]]
    except:
        gaz_df = pd.DataFrame(columns=["kanji", "lat", "lng"])

    def lookup_latlon(row):
        # è©³ç´°ã‚¨ãƒªã‚¢åã¾ãŸã¯å°ã‚¨ãƒªã‚¢åã§æ¤œç´¢
        for name in [row["detailClassName"], row["smallClassName"]]:
            for n in str(name).split("ãƒ»"):
                match = gaz_df[gaz_df["kanji"] == n]
                if not match.empty: return match.iloc[0]["lat"], match.iloc[0]["lng"]
        return None, None

    if not rakuten_df.empty:
        rakuten_df[["latitude", "longitude"]] = rakuten_df.apply(lambda r: pd.Series(lookup_latlon(r)), axis=1)
    
    return rakuten_df

rakuten_df = initialize_data()

# --- 3. æ¤œç´¢é–¢æ•° ---
def find_nearest_rakuten_area(lat, lon, rakuten_df):
    min_dist, nearest = float("inf"), None
    # åº§æ¨™ãŒå–ã‚Œã¦ã„ã‚‹ã‚¨ãƒªã‚¢ã‹ã‚‰ã®ã¿æ¤œç´¢
    valid_df = rakuten_df[rakuten_df["latitude"].notnull()]
    for _, row in valid_df.iterrows():
        dist = geodesic((lat, lon), (row["latitude"], row["longitude"])).km
        if dist < min_dist:
            min_dist, nearest = dist, row.to_dict()
    return nearest

def main_search(place_name, checkin=None, checkout=None, squeeze_cond=""):
    if not checkin: checkin = date.today().isoformat()
    if not checkout: checkout = (date.today() + timedelta(days=1)).isoformat()

    # ç‰¹æ®Šå‡¦ç†: ã‚»ã‚­ã‚¹ã‚¤ãƒã‚¤ãƒ ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¢ãƒªãƒ¼ãƒŠå¯¾ç­–
    if "ã‚»ã‚­ã‚¹ã‚¤ãƒã‚¤ãƒ " in place_name:
        # å®®åŸçœŒãƒ»ä»™å°ãƒ»æ¾å³¶ã®ã‚¨ãƒªã‚¢ã‚³ãƒ¼ãƒ‰ã‚’ç›´æŒ‡å®šï¼ˆç¢ºå®Ÿã«å®®åŸçœŒã‚’æ¤œç´¢ã•ã›ã‚‹ï¼‰
        match = {"largeClassCode": "japan", "middleClassCode": "miyagi", "smallClassCode": "A08", "detailClassCode": ""}
        print("ğŸ’¡ ã‚»ã‚­ã‚¹ã‚¤ãƒã‚¤ãƒ ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚¢ãƒªãƒ¼ãƒŠã‚’æ¤œçŸ¥: å®®åŸçœŒã‚¨ãƒªã‚¢(æ¾å³¶ãƒ»å¡©ç«ˆ)ã‚’å„ªå…ˆæ¤œç´¢ã—ã¾ã™ã€‚")
    else:
        geolocator = Nominatim(user_agent="rakuten_search_bot")
        location = geolocator.geocode(place_name + ", Japan", timeout=10)
        if not location: return pd.DataFrame()
        print(f"ğŸ“ åº§æ¨™å–å¾—: {place_name} ({location.latitude}, {location.longitude})")
        match = find_nearest_rakuten_area(location.latitude, location.longitude, rakuten_df)

    params = {
        "applicationId": RAKUTEN_APP_ID,
        "format": "json",
        "checkinDate": checkin,
        "checkoutDate": checkout,
        "largeClassCode": "japan", # å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        "middleClassCode": match["middleClassCode"],
        "smallClassCode": match["smallClassCode"],
        "detailClassCode": match.get("detailClassCode", ""),
        "squeezeCondition": squeeze_cond,
        "hits": 30
    }

    res = requests.get("https://app.rakuten.co.jp/services/api/Travel/VacantHotelSearch/20170426", params=params)
    if res.status_code != 200:
        print(f"âŒ APIã‚¨ãƒ©ãƒ¼: {res.json().get('error_description', 'Unknown Error')}")
        return pd.DataFrame()

    hotels = res.json().get("hotels", [])
    plans = []
    for h in hotels:
        hotel_data = h.get("hotel", [])
        if len(hotel_data) < 2: continue
        info = hotel_data[0]["hotelBasicInfo"]
        rooms = hotel_data[1].get("roomInfo", [])
        for i in range(0, len(rooms), 2):
            basic = rooms[i].get("roomBasicInfo", {})
            price = rooms[i+1].get("dailyCharge", {}).get("total")
            if price:
                plans.append({"ä¼šå ´": place_name, "ãƒã‚§ãƒƒã‚¯ã‚¤ãƒ³": checkin, "ãƒ›ãƒ†ãƒ«å": info["hotelName"], "æ–™é‡‘": int(price), "äºˆç´„URL": basic.get("reserveUrl")})
    return pd.DataFrame(plans)

# --- 4. å®Ÿè¡Œ ---
condition_map = {"ç¦ç…™": "kinen", "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ": "internet", "å¤§æµ´å ´": "daiyoku", "æ¸©æ³‰": "onsen", "æœé£Ÿä»˜ã": "breakfast", "å¤•é£Ÿä»˜ã": "dinner"}
squeeze_cond = ",".join([condition_map[c.strip()] for c in COND_INPUT.split(",") if c.strip() in condition_map])

venue_list = []
for line in PLACE_INPUT.splitlines():
    if not line.strip(): continue
    parts = [p.strip() for p in line.split(",")]
    if len(parts) >= 2:
        try:
            in_dt = datetime.strptime(parts[1], "%Y-%m-%d")
            out_dt = (in_dt + timedelta(days=1)).strftime("%Y-%m-%d")
            venue_list.append({"place": parts[0], "checkin": parts[1], "checkout": out_dt})
        except: venue_list.append({"place": parts[0]})
    else: venue_list.append({"place": parts[0]})

all_results = []
for v in venue_list:
    print(f"\n--- ğŸ” {v['place']} ã®æ¤œç´¢é–‹å§‹ ---")
    res_df = main_search(v["place"], v.get("checkin"), v.get("checkout"), squeeze_cond)
    if not res_df.empty: all_results.append(res_df)

if all_results:
    final_df = pd.concat(all_results).sort_values("æ–™é‡‘")
    print("\n### ğŸ¨ æ¤œç´¢çµæœä¸€è¦§")
    print(final_df.to_markdown(index=False))
    final_df.to_csv("result.csv", index=False, encoding="utf-8-sig")
else:
    print("\nâš ï¸ æ¡ä»¶ã«åˆã†ç©ºå®¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    pd.DataFrame(columns=["ä¼šå ´", "çµæœ"]).to_csv("result.csv", index=False)
